import requests
import time
import urllib3
import argparse
import sys

# å…³é—­SSLå®‰å…¨è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


def check_proxy_availability(proxy_config, timeout=10):
    """æµ‹è¯•ä»£ç†æ˜¯å¦æœ‰æ•ˆ"""
    if not proxy_config:
        print("æœªé…ç½®ä»£ç†ï¼Œè·³è¿‡ä»£ç†æµ‹è¯•\n", file=sys.stderr)
        return False

    print(f"æ­£åœ¨æµ‹è¯•ä»£ç†ï¼š{proxy_config['http']}\n")
    try:
        resp_proxy = requests.get(
            url="https://httpbin.org/ip",
            proxies=proxy_config,
            timeout=timeout,
            verify=False
        )
        resp_proxy.raise_for_status()
        proxy_ip = resp_proxy.json()["origin"]

        resp_local = requests.get(
            url="https://httpbin.org/ip",
            timeout=timeout,
            verify=False
        )
        local_ip = resp_local.json()["origin"]

        if proxy_ip != local_ip:
            print(f"âœ… ä»£ç†ç”Ÿæ•ˆï¼ä»£ç†IPï¼š{proxy_ip}\n")
            return True
        else:
            print(f"âŒ ä»£ç†æœªç”Ÿæ•ˆï¼ˆIPæœªå˜åŒ–ï¼‰ï¼Œæœ¬åœ°IPï¼š{local_ip}\n", file=sys.stderr)
            return False
    except Exception as e:
        print(f"âŒ ä»£ç†æµ‹è¯•å¤±è´¥ï¼š{str(e)}\n", file=sys.stderr)
        return False


def scrape_urls(url_list, file_path, use_proxy=False, proxy_config=None, timeout=10, total_timeout=60):
    """çˆ¬å–URLåˆ—è¡¨å¹¶å†™å…¥æ–‡ä»¶"""
    # å‚æ•°æ ¡éªŒ
    if not url_list:
        print("âŒ é”™è¯¯ï¼šURLåˆ—è¡¨ä¸ºç©ºï¼Œæ— éœ€çˆ¬å–", file=sys.stderr)
        return
    if not file_path:
        print("âŒ é”™è¯¯ï¼šæ–‡ä»¶è·¯å¾„æœªæŒ‡å®š", file=sys.stderr)
        return

    with open(file_path, mode='w', encoding='utf-8', errors='replace') as f:
        total_start_time = time.time()
        success_count = 0

        for idx, url in enumerate(url_list, 1):
            if time.time() - total_start_time >= total_timeout:
                print(f"\nâ° æ•´ä½“çˆ¬å–è¶…æ—¶ï¼ˆ{total_timeout}ç§’ï¼‰ï¼Œç»ˆæ­¢ä»»åŠ¡", file=sys.stderr)
                break

            print(f"\n[{idx}/{len(url_list)}] æ­£åœ¨çˆ¬å–ï¼š{url}")
            try:
                request_kwargs = {
                    "url": url,
                    "headers": {
                        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:129.0) Gecko/20100101 Firefox/129.0"
                    },
                    "timeout": timeout,
                    "verify": False
                }
                if use_proxy and proxy_config:
                    request_kwargs["proxies"] = proxy_config

                resp = requests.get(**request_kwargs)
                resp.raise_for_status()

                f.write(resp.text)
                f.write("\n\n")
                print(f"âœ… çˆ¬å–æˆåŠŸï¼š{url}")
                success_count += 1

            except requests.exceptions.Timeout:
                print(f"âŒ çˆ¬å–è¶…æ—¶ï¼š{url}ï¼ˆè¶…æ—¶æ—¶é—´ï¼š{timeout}ç§’ï¼‰", file=sys.stderr)
            except requests.exceptions.RequestException as e:
                print(f"âŒ çˆ¬å–å¤±è´¥ï¼š{url} é”™è¯¯ï¼š{str(e)[:50]}", file=sys.stderr)
            except Exception as e:
                print(f"âŒ æœªçŸ¥é”™è¯¯ï¼š{url} é”™è¯¯ï¼š{str(e)[:50]}", file=sys.stderr)

        print(f"\nğŸ“Š çˆ¬å–å®Œæˆï¼šå…±{len(url_list)}ä¸ªURLï¼ŒæˆåŠŸ{success_count}ä¸ªï¼Œå¤±è´¥{len(url_list) - success_count}ä¸ª")
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜è‡³ï¼š{file_path}")


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="ğŸ“¦ å…è´¹èŠ‚ç‚¹çˆ¬è™«å‘½ä»¤è¡Œå·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹ï¼š
  1. åŸºç¡€ä½¿ç”¨ï¼ˆé»˜è®¤é…ç½®ï¼‰ï¼š
     python node_scraper.py

  2. è‡ªå®šä¹‰æ–‡ä»¶è·¯å¾„å’Œä»£ç†ï¼š
     python node_scraper.py -o D:/nodes.txt -p socks5://127.0.0.1:10808

  3. è‡ªå®šä¹‰URLåˆ—è¡¨ï¼ˆå¤šä¸ªURLç”¨é€—å·åˆ†éš”ï¼‰ï¼š
     python node_scraper.py -u "https://url1.txt,https://url2.txt"

  4. ä¸ä½¿ç”¨ä»£ç†ï¼š
     python node_scraper.py --no-proxy
        """
    )

    # æ ¸å¿ƒå‚æ•°
    parser.add_argument(
        "-o", "--output",
        default="D:/yikehuj/temp/free_get_node.txt",
        help="æ–‡ä»¶ä¿å­˜è·¯å¾„ï¼ˆé»˜è®¤ï¼šD:/yikehuj/temp/free_get_node.txtï¼‰"
    )
    parser.add_argument(
        "-p", "--proxy",
        default="socks5://127.0.0.1:10808",
        help="ä»£ç†åœ°å€ï¼ˆæ ¼å¼ï¼šsocks5://IP:ç«¯å£ æˆ– http://IP:ç«¯å£ï¼Œé»˜è®¤ï¼šsocks5://127.0.0.1:10808ï¼‰"
    )
    parser.add_argument(
        "--no-proxy",
        action="store_true",
        help="ä¸ä½¿ç”¨ä»£ç†ï¼ˆä¼˜å…ˆçº§é«˜äº--proxyï¼‰"
    )
    parser.add_argument(
        "-u", "--urls",
        default="https://raw.githubusercontent.com/Barabama/FreeNodes/main/nodes/yudou66.txt,"
                "https://raw.githubusercontent.com/Barabama/FreeNodes/main/nodes/blues.txt,"
                "https://raw.githubusercontent.com/Barabama/FreeNodes/main/nodes/clashmeta.txt,"
                "https://raw.githubusercontent.com/Barabama/FreeNodes/main/nodes/nodev2ray.txt,"
                "https://raw.githubusercontent.com/Barabama/FreeNodes/main/nodes/nodefree.txt,"
                "https://raw.githubusercontent.com/Barabama/FreeNodes/main/nodes/v2rayshare.txt,"
                "https://raw.githubusercontent.com/Barabama/FreeNodes/main/nodes/wenode.txt",
        help="å¾…çˆ¬å–çš„URLåˆ—è¡¨ï¼ˆå¤šä¸ªURLç”¨è‹±æ–‡é€—å·åˆ†éš”ï¼Œé»˜è®¤ä½¿ç”¨å†…ç½®åˆ—è¡¨ï¼‰"
    )
    parser.add_argument(
        "-t", "--timeout",
        type=int,
        default=10,
        help="å•ä¸ªè¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼Œé»˜è®¤ï¼š10ï¼‰"
    )
    parser.add_argument(
        "-T", "--total-timeout",
        type=int,
        default=60,
        help="æ•´ä½“çˆ¬å–è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼Œé»˜è®¤ï¼š60ï¼‰"
    )

    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_arguments()

    # å¤„ç†ä»£ç†é…ç½®
    if args.no_proxy:
        proxy_config = None
    else:
        # ç»Ÿä¸€ä»£ç†é…ç½®æ ¼å¼ï¼ˆé€‚é…http/https/socks5ï¼‰
        proxy_config = {
            "http": args.proxy,
            "https": args.proxy
        }

    # å¤„ç†URLåˆ—è¡¨ï¼ˆæ‹†åˆ†é€—å·åˆ†éš”çš„å­—ç¬¦ä¸²ï¼‰
    url_list = [url.strip() for url in args.urls.split(",") if url.strip()]

    # æ‰“å°é…ç½®ä¿¡æ¯
    print("ğŸ“‹ å½“å‰é…ç½®ï¼š")
    print(f"  è¾“å‡ºæ–‡ä»¶ï¼š{args.output}")
    print(f"  ä»£ç†é…ç½®ï¼š{'ä¸ä½¿ç”¨ä»£ç†' if args.no_proxy else args.proxy}")
    print(f"  URLæ•°é‡ï¼š{len(url_list)}")
    print(f"  å•ä¸ªè¯·æ±‚è¶…æ—¶ï¼š{args.timeout}ç§’")
    print(f"  æ•´ä½“è¶…æ—¶ï¼š{args.total_timeout}ç§’\n")

    # æµ‹è¯•ä»£ç†
    proxy_available = check_proxy_availability(proxy_config, args.timeout)

    # å¼€å§‹çˆ¬å–
    scrape_urls(
        url_list=url_list,
        file_path=args.output,
        use_proxy=proxy_available and not args.no_proxy,
        proxy_config=proxy_config,
        timeout=args.timeout,
        total_timeout=args.total_timeout
    )

    print("\nâœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")


if __name__ == "__main__":
    main()